# 分析Ajax来抓取今日头条街拍美图

---

有时候我们在用requests抓取页面的时候，得到的结果可能和在浏览器中看到的不一样：在浏览器中可以看到正常显示的页面数据，但是使用requests得到的结果并没有。这是因为requests获取的都是原始的HTML文档，而浏览器中的页面则是经过JavaScript处理数据后生成的结果，这些数据的来源有多种，可能是通过AJax加载的，可能是包含在HTML文档中，也可能是经过JavaScript和特定算法生成的。

对于第一种情况，数据加载是一种异步加载方式，原始的页面最初不会包含某些数据，原始页面加载完后，会再向服务器请求某个接口获取数据，然后数据才被处理从而呈现到网页上，这其实就是发送一个Ajax请求。

找Web发展的趋势来看，这种形式的页面越来越多。网页的元素HTML文档不会包含任何数据，数据都是通过Ajax统一加载后在程序出来的，这样在Web开发上就可以做到前后端分离，而且降低服务器直接渲染页面代理的压力。所有，遇到这种情况，需要分析网页后台向接口发送的Ajax请求，模拟请求获取源代码。

## 什么是Ajax

全称为Asynchronous JavaScript and XML，即异步的JavaScript和XML。它不是一门编程语言，而是利用JavaScript在保证页面不被刷新、页面链接不改变的情况下与服务器交换数据并更新部分网页的技术。

基本原理：发送请求，解析内容，渲染网页

## 流程框架

1. 抓取索引页内容
2. 抓取详情页内容
3. 下载图片与保存数据库
4. 开启循环与多线程

```python
from urllib.parse import urlencode
import requests
import os
import time
from multiprocessing import Pool
from hashlib import md5


# 基础url和请求头信息
base_url = 'https://www.toutiao.com/api/search/content/?'
headers = {
    'Referer': 'https://www.toutiao.com/search/?keyword=%E5%9B%BE%E7%89%87',
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36',
    'X-Requested-With': 'XMLHttpRequest',
    'Cookie': 'tt_webid=6804026100667581959; WEATHER_CITY=%E5%8C%97%E4%BA%AC; tt_webid=6804026100667581959; csrftoken=bd43b1b47601e4d2a2e1e143418d047a; ttcid=b334bc3935ae486cacecaeb90c43c7bd27; SLARDAR_WEB_ID=6be7deee-3b7e-4fc3-b933-80b9ef3e934a; s_v_web_id=verify_k7y17xvx_JXDn3y0R_pd7H_47Ow_9dG3_yShA8wX7QWCz; __tasessionId=p53vp50rz1584578659164; tt_scid=ElNCkMU3P.LmI.6-x09lMgCsRockEtxdx5a50beaePY7m3r7ne9z4Rz7.YeDofWY1e89'
}


def get_page(offset):
    """
    获取网页数据
    :param offset: 偏移量，页码
    :return:
    """
    timestamp = int(time.time())
    params = {
        'aid': 24,
        'app_name': 'web_search',
        'offset': offset,
        'format': 'json',
        'autoload':'true',
        'keyword': '街拍',
        'count': 20,
        'en_qc': 1,
        'cur_tab': 1,
        'from': 'search_tab',
        'pd': 'synthesis',
        'timestamp': timestamp
    }
    url = base_url + urlencode(params)
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        # 获取网页数据不对会报错
        response.encoding = response.apparent_encoding
        # 编码一致
        return response.json()
    except requests.ConnectionError as e:
        print('Error', e.args)
        return None


def get_images(html):
    """
    获取图片标题和链接
    :param html: 网页数据
    :return:
    """
    if html.get('data'):
        # 判断是否有data标签
        html = html.get('data')
        for item in html:
            # 先判断是否有标题和图片链接标签，才能进行下面操作，不然会报NoneType错误
            if item.get('title'):
                title = item.get('title').replace(' |', ' ').replace('\\', ' ')
            if item.get('image_list'):
                for image_urls in item.get('image_list'):
                    # 获得image_list里每一条的图片
                    image_url = image_urls['url']
                    yield {
                        'title': title,
                        'image': image_url
                    }
    else:
        print('没有获取到数据')


def save_image(item):
    """
    保存图片
    :param item: 标题和图片链接
    :return:
    """
    img_path = 'images' + '/' + item.get('title')
    # 设置图片保存路径
    if not os.path.exists(img_path):
        try:
            os.mkdir(img_path)
        except OSError:
            # 这里有报错，没有解决
            print('文件命名错误')
    try:
        response = requests.get(item.get('image'))
        response.raise_for_status()
        file_path = '{0}/{1}.{2}'.format(img_path, md5(response.content).hexdigest(), 'jpg')
        if not os.path.exists(file_path):
            with open(file_path, 'wb') as f:
                f.write(response.content)
        else:
            print('已经存在', file_path)
    except requests.ConnectionError:
        print('下载不成功')
    except OSError:
        # 没有解决有些图片下载下来文件夹名是乱码，只能跳过
        print('图片名称格式不对')


def main(offset):
    """
    主程序执行
    :param offset: ajax分页请求参数
    :return:
    """
    html = get_page(offset)
    # 获取网页数据
    for item in get_images(html):
        # 获取图片标题和图片链接
        save_image(item)
        # 保存图片


GROUP_START = 0
GROUP_END = 10


if __name__ == '__main__':
    pool = Pool()
    group = ([x * 20 for x in range(GROUP_START, GROUP_END + 1)])
    pool.map(main, group)
    # 多线程爬取
    pool.close()
    pool.join()
```

